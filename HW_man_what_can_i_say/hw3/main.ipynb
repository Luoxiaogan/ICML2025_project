{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业的目标是通过手动实现神经网络的前向输出和反向梯度计算及参数更新过程，加深对神经网络前反向传播的理解。我们以多层MLP为例展开，第一部分将构建模型并检验手动计算的正确性，第二部分将以一个实例来观察模型的训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业需要安装pytorch库（深度学习框架）与matplotlib库（绘图工具），确保已经安装完毕后，可运行如下导入语句开始本次作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、建构模型、实现手动求导并检验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步，我们将构造一个多层感知机（MLP）模型，并定义其前向（forward）和反向（backward）过程。\n",
    "\n",
    "一个L层MLP的模型结构是 {Affine -> Activation} x (L - 1) -> Affine，这里为了简便省略了layer norm和dropout层。其中Affine是仿射层，Activation是激活函数，这里我们以Sigmoid作为代表。\n",
    "\n",
    "一个$L$层的多层感知机模型可以表示为如下形式：\n",
    "$$\n",
    "    x\\xrightarrow{W_1}h_1\\xrightarrow{\\sigma(\\cdot)}z_1\\xrightarrow{W_2}h_2\\xrightarrow{\\sigma(\\cdot)}z_2\\cdots\\xrightarrow{W_L}h_L\\xrightarrow{f(\\cdot)}l,\n",
    "$$\n",
    "其中$x\\in\\mathbb{R}^{d_0},\\,h_i\\in\\mathbb{R}^{d_i},\\,z_k\\in\\mathbb{R}^{d_i},\\,W_i\\in\\mathbb{R}^{d_{i-1},d_i},\\,\\sigma(\\cdot)$为Sigmoid激活函数，$f(\\cdot):\\mathbb{R}^{d_L}\\rightarrow\\mathbb{R}$为可求导的损失函数。\n",
    "\n",
    "下面我们开始构建模型。在pytorch中，我们通常通过定义`torch.nn.Module`的子类来建构模型。\n",
    "\n",
    "父类`torch.nn.Module`主要的特性是其使用`.forward`方法重载了魔法方法`.__call`，这表示，如果直接将实例名称像函数一样调用，实现的就是其`forward`方法。\n",
    "\n",
    "`nn.Linear`为pytorch的一个内置的`Module`子类，需要声明输入维度和输出维度，它内含了两个可学习参数——权重矩阵`weight`与偏置`bias`，并默认其`requires_grad=True`，表示其需要求梯度，即它们是可学习的而非固定的参数，这两个参数在未被修改前是随机初始化的，取决于计算机的内存残余数据。\n",
    "\n",
    "在下面的代码中，请根据参考的初始化网络参数函数，补充`forward`和`backward`函数。\n",
    "\n",
    "`forward`函数是网络的前向传播，输出模型的预测，为了实现手动求导，我们在模型初始化时提供了用于缓存激活值的字典`activations`（激活值），它并不仅代表激活函数后的输出，而是所有计算图中可能需要的中间计算值，你可以在定义`forward`函数的同时缓存你需要的中间值。\n",
    "\n",
    "`backward`函数是我们额外定义的网络的反向传播方法，用于实现手动计算梯度。它以模型输出$h_L$的梯度为输入，同时根据模型参数、前向传播时缓存的激活值、已计算出的每一层激活值的梯度等计算每一层参数的梯度。\n",
    "\n",
    "在书写过程中，你需要注意：pytorch中一维向量默认为行向量，你可以使用`@`, `torch.matmul`, `torch.einsum`等多种形式实现矩阵乘积，但一定要保证维度对应正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(nn.Linear(in_features=prev_dim, out_features=hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=prev_dim, out_features=output_dim)\n",
    "\n",
    "        # 缓存激活值的字典\n",
    "        self.activations = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 缓存输入值，使用detach()方法使其从计算图中分离。\n",
    "        self.activations[\"input\"] = x.detach()\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: 前向传播，根据网络结构计算：{Affine -> Sigmoid} x (L - 1) -> Affine，将激活值保存在self.activations中\n",
    "        # 例如第一层，self.activations[\"fc0\"]和self.activations[\"sigmoid0\"]分别保存线性层和激活函数的输出\n",
    "        ####################################################################################################\n",
    "\n",
    "        # 隐藏层\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            self.activations[f\"fc{i}\"] = x.detach()\n",
    "            x = F.sigmoid(x)\n",
    "            self.activations[f\"sigmoid{i}\"] = x.detach()\n",
    "\n",
    "        # 输出层\n",
    "        x = self.output_layer(x)\n",
    "        self.activations[\"output\"] = x.detach()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: 反向传播，根据前向传播和缓存激活值计算梯度，将其保存在layer.weight.grad和layer.bias.grad中\n",
    "        # 例如第一层，self.layers[0].weight.grad和self.layers[0].bias.grad分别保存权重和偏置的梯度\n",
    "        ####################################################################################################\n",
    "\n",
    "        # 输出层梯度，请将 None 修改为你计算的结果。\n",
    "        grad = grad_output\n",
    "\n",
    "        output_input = self.activations[f\"sigmoid{len(self.layers)-1}\"]\n",
    "        self.output_layer.weight.grad = grad.T @ output_input\n",
    "        self.output_layer.bias.grad = grad.sum(dim=0)\n",
    "\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            sigmoid_output = self.activations[f\"sigmoid{i}\"]\n",
    "            grad = grad @ self.output_layer.weight if i == len(self.layers) - 1 else grad @ self.layers[i + 1].weight\n",
    "            sigmoid_grad = sigmoid_output * (1 - sigmoid_output)\n",
    "            grad = grad * sigmoid_grad\n",
    "\n",
    "            if i > 0:\n",
    "                layer_input = self.activations[f\"sigmoid{i-1}\"]\n",
    "            else:\n",
    "                layer_input = self.activations[\"input\"]\n",
    "            \n",
    "            self.layers[i].weight.grad = grad.T @ layer_input\n",
    "            self.layers[i].bias.grad = grad.sum(dim=0)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们通过与自动梯度计算方法比较来检查上述实现的前向和反向过程是否正确，这里使用均方误差（MSE），其计算公式为：\n",
    "$$\n",
    "    l(y_{\\mathrm{pred}},y_{\\mathrm{true}})=\\frac{1}{d}\\|y_{\\mathrm{pred}}-y_{\\mathrm{true}}\\|_2^2,\n",
    "$$\n",
    "其中 $d$ 为 $y_{\\mathrm{true}}$ 的维数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 模型和数据\n",
    "mlp = CustomMLP(input_dim=32, hidden_dims=[16, 4], output_dim=2)\n",
    "\n",
    "# 使用copy方法确保模型初始参数一致\n",
    "mlp_manual = CustomMLP(input_dim=32, hidden_dims=[16, 4], output_dim=2)\n",
    "mlp_autograd = copy.deepcopy(mlp_manual)\n",
    "\n",
    "# 生成随机数据\n",
    "x = torch.randn(32)\n",
    "y_true = torch.randn(2) \n",
    "\n",
    "# 前向传播\n",
    "y_pred_manual = mlp_manual(x)\n",
    "y_pred_autograd = mlp_autograd(x)\n",
    "\n",
    "# 计算均方误差\n",
    "loss_manual = F.mse_loss(y_pred_manual, y_true)\n",
    "loss_autograd = F.mse_loss(y_pred_autograd, y_true)\n",
    "\n",
    "print(\"Output Error:\", torch.norm(y_pred_manual - y_pred_autograd).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于手动定义的backward方法，我们先要计算损失对于模型输出的梯度$\\frac{\\partial l}{\\partial y_{\\mathrm{pred}}}$，然后将该梯度传入`backward`进行反向传播；对于自动计算，使用loss.backward()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245902/3803536256.py:49: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3697.)\n",
      "  self.output_layer.weight.grad = grad.T @ output_input\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [2] and src [4] to have the same number of elements, but got 2 and 4 elements respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m grad_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m output_dim \u001b[38;5;241m*\u001b[39m (y_pred_manual \u001b[38;5;241m-\u001b[39m y_true)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 其余层的梯度使用先前定义的bachward方法计算\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmlp_manual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 使用autograd计算梯度\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss_autograd\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mCustomMLP.backward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     46\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad_output\n\u001b[1;32m     48\u001b[0m output_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_input\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [2] and src [4] to have the same number of elements, but got 2 and 4 elements respectively"
     ]
    }
   ],
   "source": [
    "# 手动计算梯度\n",
    "####################################################################################################\n",
    "# TODO: 计算出模型输出层的梯度 dloss/dy_pred\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "output_dim = y_pred_manual.shape[0]\n",
    "grad_output = 2.0 / output_dim * (y_pred_manual - y_true)\n",
    "\n",
    "# 其余层的梯度使用先前定义的bachward方法计算\n",
    "mlp_manual.backward(grad_output)\n",
    "\n",
    "# 使用autograd计算梯度\n",
    "loss_autograd.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先检查梯度形状：例如，对于第一层，其权重梯度 fc0.weight.grad 的形状应为（hidden_dims[0], input_dim),偏置梯度 fc0.bias.grad 的形状应为（hidden_dims[0]）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印各层梯度的形状\n",
    "for i, layer in enumerate(mlp_manual.layers):\n",
    "    print(f\"fc{i}.weight.grad shape: {layer.weight.grad.shape}\")\n",
    "    print(f\"fc{i}.bias.grad shape: {layer.bias.grad.shape}\")\n",
    "print(f\"output_layer.weight.grad shape: {mlp_manual.output_layer.weight.grad.shape}\")\n",
    "print(f\"output_layer.bias.grad shape: {mlp_manual.output_layer.bias.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察各层权重和偏置的梯度差异。如果你实现的backward函数正确，所有的误差不应超过1e-6。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (layer_manual, layer_auto) in enumerate(zip(mlp_manual.layers, mlp_autograd.layers)):\n",
    "    print(f\"fc{i}.weight.grad error: {torch.norm(layer_manual.weight.grad - layer_auto.weight.grad).item()}\")\n",
    "    print(f\"fc{i}.bias.grad error: {torch.norm(layer_manual.bias.grad - layer_auto.bias.grad).item()}\")\n",
    "\n",
    "print(f\"output_layer.weight.grad error: {torch.norm(mlp_manual.output_layer.weight.grad - mlp_autograd.output_layer.weight.grad).item()}\")\n",
    "print(f\"output_layer.bias.grad error: {torch.norm(mlp_manual.output_layer.bias.grad - mlp_autograd.output_layer.bias.grad).item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实例：MNIST数据集分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST数据集是一个广泛使用的机器学习数据集，主要用于手写数字识别任务。它包含了28x28像素的灰度图像，这些图像涵盖了从0到9的十个数字类别，每个类别包含了大量的手写样本。整个数据集分为训练集和测试集两部分，其中训练集包含60,000个样本，用于模型训练；测试集包含10,000个样本，用于评估模型性能。MNIST数据集由于其相对简单且易于理解的特点，常被用作算法原型设计和初步验证的基准数据集。\n",
    "\n",
    "请你运行该部分的代码，同时学习pytorch深度学习实验的基本模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 版本:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 将图片展平，转换为tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# 使用DataLoader加载数据\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dims = [256]\n",
    "output_dim = 10\n",
    "\n",
    "model = CustomMLP(input_dim, hidden_dims, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意和上面不同的是，对于分类任务我们采用了交叉熵损失（cross entropy）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "num_epochs = 25\n",
    "learning_rate = 0.01\n",
    "loss_history = []\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # 前向传播\n",
    "        outputs = model.forward(images)\n",
    "\n",
    "        # 计算交叉熵损失\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失函数下降曲线\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, num_epochs + 1), loss_history, linestyle='-')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model.forward(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
